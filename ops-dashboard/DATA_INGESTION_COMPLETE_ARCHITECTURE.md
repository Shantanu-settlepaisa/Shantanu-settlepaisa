# Complete Data Ingestion Architecture
## Current State + Future Streaming (Debezium + Kafka + Flink)

**Date:** October 2, 2025  
**Status:** âœ… Foundation Ready, ğŸš§ Streaming Pipeline Planned

---

## ğŸ¯ Executive Summary

You have **ALREADY built** the foundation for streaming architecture:

âœ… **Kafka Infrastructure** - Docker Compose ready  
âœ… **SFTP Bank Ingestion** - Automated bank file polling  
âœ… **PG Ingestion API** - Webhook-based transaction ingestion  
âœ… **SabPaisa Connector** - Direct DB access for transactions  
â³ **Debezium CDC** - NOT YET SETUP  
â³ **Apache Flink** - NOT YET SETUP  

---

## ğŸ“Š Current Data Ingestion Flows (ALREADY WORKING)

### **Flow 1: SFTP Bank File Ingestion** âœ…

```
Bank SFTP Servers
    â†“ (Poll every 60s)
SFTP Watcher Service (Port 5106)
    â”œâ”€ AXIS: 3 cutoffs (11:30, 15:30, 20:00)
    â”œâ”€ HDFC: 4 cutoffs (10:00, 14:00, 18:00, 22:00)
    â””â”€ ICICI: 2 cutoffs (09:30, 21:30)
    â†“ (Download & Validate)
Staging Directory (/tmp/sftp-staging)
    â†“ (Parse CSV)
sp_v2_bank_files_ingested (tracking)
sp_v2_bank_file_expectations (expected vs received)
    â†“ (Insert data)
sp_v2_transactions (bank data)
```

**Tables Created:**
```sql
-- Migration: 003_create_ingestion_tables.sql
sp_v2_bank_configs              -- Bank SFTP configurations
sp_v2_bank_files_ingested       -- Downloaded file tracking
sp_v2_bank_file_expectations    -- Expected files per cutoff
sp_v2_sftp_connector_health     -- Health status
```

**Status:** âœ… Fully implemented, waiting for bank SFTP credentials

---

### **Flow 2: PG Gateway Webhook Ingestion** âœ…

```
Payment Gateway APIs (Razorpay, PayU, Paytm)
    â†“ (Webhook POST)
PG Ingestion Server (Port 5111)
    â”œâ”€ /webhook/razorpay
    â”œâ”€ /webhook/payu
    â””â”€ /webhook/paytm
    â†“ (Validate signature)
Deduplication Check (in-memory Set)
    â†“ (Insert if unique)
sp_v2_transactions_v1 (PG transaction data)
```

**Features:**
- Webhook signature validation
- Rate limiting (1000 req/min)
- Deduplication (prevent double processing)
- Scheduled polling fallback (every 5 minutes)

**Status:** âœ… Fully implemented, can receive webhooks

---

### **Flow 3: SabPaisa Direct Connector** âœ…

```
SabPaisa Core DB (3.108.237.99:5432)
    â†“ (Direct SQL Query)
SabPaisa Connector Service
    â”œâ”€ fetchPGTransactions(cycleDate)
    â”œâ”€ fetchBankStatements(cycleDate)
    â””â”€ fetchMerchantData()
    â†“ (Transform)
V2 Database (settlement analytics queries)
```

**Status:** âœ… Used for merchant config sync, can query transactions

---

## ğŸš€ Future: Streaming Architecture (Debezium + Kafka + Flink)

### **Why Add Streaming When You Have Ingestion?**

**Current Limitations:**
1. âŒ **SFTP is Batch** - Files arrive hourly, not real-time
2. âŒ **Webhooks are Unreliable** - Can miss, fail, or arrive late
3. âŒ **Polling is Wasteful** - Repeatedly query same data
4. âŒ **No Historical Replay** - Can't reprocess past events
5. âŒ **Single Consumer** - Only one service can process

**Streaming Benefits:**
1. âœ… **Real-Time CDC** - Capture every DB change (< 1s lag)
2. âœ… **Guaranteed Delivery** - Kafka persistence & replay
3. âœ… **Multi-Consumer** - Analytics, ML, Alerts all consume same stream
4. âœ… **Transform Layer** - Flink enriches/aggregates before storage
5. âœ… **Event Sourcing** - Complete audit trail of all changes

---

## ğŸ—ï¸ Complete Future Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA SOURCES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. SabPaisa Core DB (PostgreSQL)                            â”‚
â”‚    â€¢ transactions_to_settle (millions of rows)              â”‚
â”‚    â€¢ merchant_data (15K merchants)                          â”‚
â”‚    â€¢ payment_gateway_response                               â”‚
â”‚                                                             â”‚
â”‚ 2. Bank SFTP Servers                                        â”‚
â”‚    â€¢ AXIS (3 cutoffs/day)                                   â”‚
â”‚    â€¢ HDFC (4 cutoffs/day)                                   â”‚
â”‚    â€¢ ICICI (2 cutoffs/day)                                  â”‚
â”‚                                                             â”‚
â”‚ 3. PG Gateway APIs                                          â”‚
â”‚    â€¢ Razorpay webhooks                                      â”‚
â”‚    â€¢ PayU webhooks                                          â”‚
â”‚    â€¢ Paytm webhooks                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INGESTION LAYER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ A. CDC (Change Data Capture) - NEW                         â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚    â”‚  Debezium PostgreSQL Connector                     â”‚  â”‚
â”‚    â”‚  â€¢ Reads PostgreSQL WAL (Write-Ahead Log)          â”‚  â”‚
â”‚    â”‚  â€¢ Captures INSERT/UPDATE/DELETE                   â”‚  â”‚
â”‚    â”‚  â€¢ Converts to Kafka events                        â”‚  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚ B. File Ingestion - EXISTING âœ…                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚    â”‚  SFTP Watcher (Port 5106)                          â”‚  â”‚
â”‚    â”‚  â€¢ Polls bank SFTP every 60s                       â”‚  â”‚
â”‚    â”‚  â€¢ Downloads files                                 â”‚  â”‚
â”‚    â”‚  â€¢ Publishes to Kafka: bank.files.received        â”‚  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚ C. API Ingestion - EXISTING âœ…                              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚    â”‚  PG Webhook Server (Port 5111)                     â”‚  â”‚
â”‚    â”‚  â€¢ Receives PG webhooks                            â”‚  â”‚
â”‚    â”‚  â€¢ Validates signatures                            â”‚  â”‚
â”‚    â”‚  â€¢ Publishes to Kafka: pg.transactions            â”‚  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              APACHE KAFKA (Event Streaming)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Topics (Already configured âœ…):                             â”‚
â”‚  â€¢ recon.jobs                                               â”‚
â”‚  â€¢ recon.results                                            â”‚
â”‚  â€¢ payment.events                                           â”‚
â”‚                                                             â”‚
â”‚ Topics (NEW for streaming):                                 â”‚
â”‚  â€¢ sabpaisa.public.transactions_to_settle (Debezium CDC)    â”‚
â”‚  â€¢ sabpaisa.public.merchant_data (Debezium CDC)             â”‚
â”‚  â€¢ bank.files.received (SFTP watcher)                       â”‚
â”‚  â€¢ pg.transactions (PG webhooks)                            â”‚
â”‚  â€¢ settlement.batches.created (V2 events)                   â”‚
â”‚  â€¢ settlement.batches.approved (V2 events)                  â”‚
â”‚                                                             â”‚
â”‚ Infrastructure (Docker Compose):                            â”‚
â”‚  âœ… Zookeeper (Port 2181)                                   â”‚
â”‚  âœ… Kafka Broker (Port 9092)                                â”‚
â”‚  âœ… Kafka UI (Port 8090)                                    â”‚
â”‚  â³ Kafka Connect (for Debezium) - Need to add             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         APACHE FLINK (Stream Processing) - NEW              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Job 1: Transaction Enrichment                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Input:  sabpaisa.transactions (raw)                  â”‚   â”‚
â”‚  â”‚ Join:   merchant_master (dimension table)            â”‚   â”‚
â”‚  â”‚ Enrich: Add merchant name, settlement cycle          â”‚   â”‚
â”‚  â”‚ Output: enriched.transactions â†’ V2 DB               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚ Job 2: Real-Time Aggregates                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Window: Tumbling 1-hour                              â”‚   â”‚
â”‚  â”‚ Compute: GMV, transaction count, settlement rate     â”‚   â”‚
â”‚  â”‚ Output: sp_v2_realtime_aggregates (materialized)    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚ Job 3: Reconciliation Matcher                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Join: PG transactions + Bank files (by UTR)          â”‚   â”‚
â”‚  â”‚ Match: Amount, date, merchant                        â”‚   â”‚
â”‚  â”‚ Output: recon.matched / recon.unmatched             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚ Job 4: Settlement Trigger                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Rule: If T+settlement_cycle days passed              â”‚   â”‚
â”‚  â”‚ Trigger: settlement.jobs.pending                     â”‚   â”‚
â”‚  â”‚ Action: Call settlement calculator V3                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STORAGE LAYER (V2 Database)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tables:                                                     â”‚
â”‚  â€¢ sp_v2_transactions (enriched from Flink)                 â”‚
â”‚  â€¢ sp_v2_settlement_batches (settlement results)            â”‚
â”‚  â€¢ sp_v2_realtime_aggregates (materialized views)           â”‚
â”‚  â€¢ sp_v2_reconciliation_status (match results)              â”‚
â”‚  â€¢ sp_v2_bank_files_ingested (SFTP tracking)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CONSUMERS                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Settlement Analytics API (Port 5107) - Fast queries     â”‚
â”‚ 2. Real-Time Alerts Service - Anomaly detection            â”‚
â”‚ 3. Data Warehouse Sync - Snowflake/BigQuery                â”‚
â”‚ 4. ML Feature Store - Model training                        â”‚
â”‚ 5. Audit Trail Service - Compliance reporting               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ How Debezium Works (Your Question)

### **Debezium = Change Data Capture (CDC)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      SabPaisa PostgreSQL Database                   â”‚
â”‚                                                     â”‚
â”‚  INSERT INTO transactions_to_settle VALUES (...);   â”‚
â”‚                                                     â”‚
â”‚  PostgreSQL writes to Write-Ahead Log (WAL)         â”‚
â”‚  â†“                                                  â”‚
â”‚  WAL: {operation: INSERT, table: transactions_...} â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ (Debezium reads WAL)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Debezium PostgreSQL Connector                â”‚
â”‚                                                     â”‚
â”‚  1. Connects to PostgreSQL                          â”‚
â”‚  2. Creates replication slot                        â”‚
â”‚  3. Reads WAL in real-time                          â”‚
â”‚  4. Converts DB changes â†’ Kafka events              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“ (Publishes event)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Kafka Topic                            â”‚
â”‚  sabpaisa.public.transactions_to_settle             â”‚
â”‚                                                     â”‚
â”‚  Event: {                                           â”‚
â”‚    "before": null,                                  â”‚
â”‚    "after": {                                       â”‚
â”‚      "txn_id": "TXN123",                            â”‚
â”‚      "merchant_code": "KAPR63",                     â”‚
â”‚      "amount": "5000.00",                           â”‚
â”‚      "txn_date": "2025-10-02",                      â”‚
â”‚      "status": "SUCCESS"                            â”‚
â”‚    },                                               â”‚
â”‚    "op": "c",  // create                            â”‚
â”‚    "ts_ms": 1727870400000                           â”‚
â”‚  }                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Points:**
- âœ… **No Code Changes** in SabPaisa DB
- âœ… **Real-Time** (< 1 second lag)
- âœ… **Non-Intrusive** (reads WAL, doesn't query tables)
- âœ… **Guaranteed Order** (preserves transaction order)
- âœ… **At-Least-Once Delivery** (Kafka guarantees)

---

## ğŸ”§ How Kafka Works (Your Question)

### **Kafka = Distributed Event Log**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Kafka Broker                         â”‚
â”‚                                                      â”‚
â”‚  Topic: sabpaisa.transactions                        â”‚
â”‚  â”œâ”€ Partition 0: [event1, event2, event3, ...]      â”‚
â”‚  â”œâ”€ Partition 1: [event4, event5, event6, ...]      â”‚
â”‚  â””â”€ Partition 2: [event7, event8, event9, ...]      â”‚
â”‚                                                      â”‚
â”‚  Retention: 7 days (configurable)                    â”‚
â”‚  Replication: 3 copies (configurable)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“                    â†“                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Consumer 1     â”‚  â”‚  Consumer 2     â”‚  â”‚  Consumer 3     â”‚
â”‚  (Flink Job)    â”‚  â”‚  (Analytics)    â”‚  â”‚  (Alerts)       â”‚
â”‚                 â”‚  â”‚                 â”‚  â”‚                 â”‚
â”‚  Reads from     â”‚  â”‚  Reads from     â”‚  â”‚  Reads from     â”‚
â”‚  offset 0       â”‚  â”‚  offset 100     â”‚  â”‚  offset 50      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Features:**
- âœ… **Multiple Consumers** (same event read by many services)
- âœ… **Replay** (can go back to offset 0 and reprocess)
- âœ… **Persistence** (stores events for 7 days)
- âœ… **Scalable** (add more partitions for throughput)
- âœ… **Fault-Tolerant** (replicates across brokers)

**Already Setup:**
```yaml
# docker-compose.kafka.yml
services:
  zookeeper:  # Kafka metadata coordinator
  kafka:      # Event broker
  kafka-ui:   # Management UI (localhost:8090)
```

---

## ğŸ”§ How Flink Works (Your Question)

### **Flink = Stream Processing Framework**

```sql
-- Example Flink SQL Job
CREATE TABLE sabpaisa_transactions (
  txn_id STRING,
  merchant_code STRING,
  amount DECIMAL(15,2),
  txn_date DATE
) WITH (
  'connector' = 'kafka',
  'topic' = 'sabpaisa.public.transactions_to_settle',
  'properties.bootstrap.servers' = 'localhost:9092',
  'format' = 'debezium-json'
);

CREATE TABLE merchant_master (
  merchant_id STRING,
  merchant_name STRING
) WITH (
  'connector' = 'jdbc',
  'url' = 'jdbc:postgresql://localhost:5433/settlepaisa_v2',
  'table-name' = 'sp_v2_merchant_master'
);

-- Real-time enrichment
INSERT INTO enriched_transactions
SELECT 
  t.txn_id,
  t.merchant_code,
  m.merchant_name,  -- Enriched!
  t.amount,
  t.txn_date
FROM sabpaisa_transactions t
LEFT JOIN merchant_master m 
  ON t.merchant_code = m.merchant_id;

-- Real-time aggregate (hourly GMV)
SELECT 
  TUMBLE_START(txn_time, INTERVAL '1' HOUR) as window_start,
  merchant_code,
  COUNT(*) as txn_count,
  SUM(amount) as gmv
FROM sabpaisa_transactions
GROUP BY 
  TUMBLE(txn_time, INTERVAL '1' HOUR),
  merchant_code;
```

**Capabilities:**
- âœ… **Stream Joins** (join Kafka streams with DB tables)
- âœ… **Windowing** (compute hourly/daily aggregates)
- âœ… **CEP** (Complex Event Processing - detect patterns)
- âœ… **State Management** (remember previous events)
- âœ… **Exactly-Once** (guarantees for critical operations)

---

## ğŸ“‹ What You Need to Add

### **Step 1: Add Kafka Connect (Debezium Runtime)**

Update `docker-compose.kafka.yml`:
```yaml
  kafka-connect:
    image: debezium/connect:2.4
    hostname: kafka-connect
    container_name: settlepaisa-kafka-connect
    depends_on:
      - kafka
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: connect_configs
      OFFSET_STORAGE_TOPIC: connect_offsets
```

### **Step 2: Configure Debezium Connector**

```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "sabpaisa-transactions-connector",
    "config": {
      "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
      "database.hostname": "3.108.237.99",
      "database.port": "5432",
      "database.user": "settlepaisainternal",
      "database.password": "sabpaisa123",
      "database.dbname": "settlepaisa",
      "database.server.name": "sabpaisa",
      "table.include.list": "public.transactions_to_settle,public.merchant_data",
      "topic.prefix": "sabpaisa",
      "plugin.name": "pgoutput"
    }
  }'
```

### **Step 3: Add Apache Flink**

```yaml
  flink-jobmanager:
    image: flink:1.17
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager

  flink-taskmanager:
    image: flink:1.17
    depends_on:
      - flink-jobmanager
    command: taskmanager
    environment:
      FLINK_PROPERTIES: |
        jobmanager.rpc.address: flink-jobmanager
```

---

## âœ… What's Already Working

| Component | Status | Port | Purpose |
|-----------|--------|------|---------|
| Kafka Broker | âœ… | 9092 | Event streaming |
| Zookeeper | âœ… | 2181 | Kafka coordination |
| Kafka UI | âœ… | 8090 | Management interface |
| SFTP Watcher | âœ… | 5106 | Bank file ingestion |
| PG Webhook Server | âœ… | 5111 | Gateway webhooks |
| SabPaisa Connector | âœ… | N/A | Direct DB queries |
| Debezium | â³ | 8083 | CDC (need to add) |
| Flink | â³ | 8081 | Stream processing (need to add) |

---

## ğŸ¯ Recommended Next Steps

### **For Settlement Analytics (Now - 4-5 hours):**
âœ… Build Analytics API querying V2 tables  
âœ… Use current test/manual upload data  
âœ… Power all 9 analytics tiles  
âœ… Ready to demo

### **For Streaming Pipeline (Later - 1-2 weeks):**
1. Add Debezium to docker-compose
2. Configure CDC for SabPaisa tables
3. Set up Flink jobs for enrichment
4. Test end-to-end streaming
5. Cut over to production

### **Result:**
- âœ… Analytics works with test data (now)
- âœ… When streaming goes live, analytics gets real data (automatic)
- âœ… No code changes needed in analytics layer

---

**Should I proceed with building Settlement Analytics API now against the current V2 schema?**
